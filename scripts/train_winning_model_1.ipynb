{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mathieu/Prose/Mathieu/Benter-Project\n"
     ]
    }
   ],
   "source": [
    "#%cd C:/Users/Mathieu/Desktop/Projets/Benter\n",
    "%cd /home/mathieu/Prose/Mathieu/Benter-Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathieu/.pyenv/versions/3.7.9/envs/benter-project_venv/lib/python3.7/site-packages/tqdm/std.py:699: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from itertools import combinations\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "from scipy.stats import rankdata\n",
    "import scipy\n",
    "import json\n",
    "\n",
    "from utils import import_data\n",
    "from winning_validation import errors\n",
    "from winning_validation import r_squared\n",
    "from winning_horse_models import sklearn\n",
    "from winning_horse_models.dl_shared_layers import LogisticRegressionModel, DLSharedLayersModel\n",
    "from winning_horse_models.xgboost import XGBoostWinningModel\n",
    "from winning_horse_models.lgbm import LGBMWinningModel\n",
    "from training_procedures import sequential_training, flattened_training\n",
    "from constants import Sources\n",
    "from utils import preprocess\n",
    "\n",
    "from database.setup import create_sqlalchemy_session\n",
    "from models.race import Race\n",
    "from models.runner import Runner\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = Sources.UNIBET\n",
    "N_FEATURES = preprocess.get_n_preprocessed_feature_columns(source=SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 618 µs, sys: 194 µs, total: 812 µs\n",
      "Wall time: 818 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture --no-stdout\n",
    "\n",
    "#xgboost_winning_model, training_history = flattened_training.train_per_n_horses_races(source=SOURCE, winning_model=XGBoostWinningModel(source=SOURCE), verbose=True)\n",
    "#xgboost_winning_model.save_model(prefix=\"48_col_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_winning_model=XGBoostWinningModel.load_model(prefix=\"48_col_\", source=SOURCE, n_features = N_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.75 s, sys: 106 ms, total: 8.86 s\n",
      "Wall time: 8.24 s\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "%%time\n",
    "res=r_squared.compute_mcfadden_r_squared_on_n_horses(source=SOURCE,winning_model=xgboost_winning_model, n_horses=10,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06956215250069908"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['model_r_squared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(silent=False, \n",
    "                      scale_pos_weight=1,\n",
    "                      learning_rate=0.01,  \n",
    "                      colsample_bytree = 0.4,\n",
    "                      subsample = 0.8,\n",
    "                      objective='binary:logistic', \n",
    "                      n_estimators=1000, \n",
    "                      reg_alpha = 0.3,\n",
    "                      max_depth=4, \n",
    "                      gamma=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/hyperparameter-tuning-hyperopt-bayesian-optimization-for-xgboost-and-neural-network-8aedf278a1c9\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 2, 10, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.2,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': hp.quniform('n_estimators', 20,180,1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Classifier:\n",
    "def hyperparameter_tuning(space):\n",
    "    for param in ('max_depth', 'n_estimators', 'reg_alpha', 'min_child_weight'):\n",
    "        if param in space:\n",
    "            space[param] = int(space[param])\n",
    "            \n",
    "    for param in ('gamma', 'reg_lambda', 'colsample_bytree'):\n",
    "        if param in space:\n",
    "            space[param] = float(space[param])  \n",
    "    model =XGBoostWinningModel(source=SOURCE, n_features = N_FEATURES, hyperparameters=space)\n",
    "    \n",
    "    model, _ = flattened_training.train_on_n_horses_races(source=SOURCE, winning_model=model, n_horses=10, verbose=True)\n",
    "\n",
    "    res=r_squared.compute_mcfadden_r_squared_on_n_horses(source=SOURCE,winning_model=model, n_horses=10, verbose=True)\n",
    "    r_squared__score= res['model_r_squared']\n",
    "    print (f\"R²: {r_squared__score:.2}, {space}\")\n",
    "    #change the metric if you like\n",
    "    return {'loss': -r_squared__score, 'status': STATUS_OK, 'model': model}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:51:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.218, val loss per horse: 0.220 Train Accuracy: 23.4%, Val Accuracy: 20.9%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)\n",
      "Mean Predicted probas of actual race result: 11.471% (Random: 9.999%, Odds: 20.823%)\n",
      "On 2196 races with 10 horses,R² of winning model: 0.04, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.044, {'colsample_bytree': 0.867336526984638, 'gamma': 7.370804695882834, 'max_depth': 9, 'min_child_weight': 6, 'n_estimators': 33, 'reg_alpha': 48, 'reg_lambda': 0.6757482952936152}\n",
      "[07:53:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.228, val loss per horse: 0.229 Train Accuracy: 17.8%, Val Accuracy: 16.4%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 10.168% (Random: 9.841%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.01, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.0058, {'colsample_bytree': 0.28524468919335294, 'gamma': 7.184975886454965, 'max_depth': 5, 'min_child_weight': 8, 'n_estimators': 145, 'reg_alpha': 128, 'reg_lambda': 0.7561849720082191}\n",
      "[07:55:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.206, val loss per horse: 0.215 Train Accuracy: 29.0%, Val Accuracy: 22.2%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 12.543% (Random: 10.197%, Odds: 20.823%)  \n",
      "On 2196 races with 10 horses,R² of winning model: 0.06, R² of odds: 0.19, [R² of random model: -0.12 (should be closed to 0)]\n",
      "R²: 0.063, {'colsample_bytree': 0.5941283685804688, 'gamma': 2.520147211564109, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 176, 'reg_alpha': 46, 'reg_lambda': 0.23407568398499623}\n",
      "[07:59:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.219, val loss per horse: 0.220 Train Accuracy: 23.0%, Val Accuracy: 20.6%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)       \n",
      "Mean Predicted probas of actual race result: 11.341% (Random: 9.903%, Odds: 20.823%)  \n",
      "On 2196 races with 10 horses,R² of winning model: 0.04, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.042, {'colsample_bytree': 0.58686095799213, 'gamma': 5.835397078124499, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 112, 'reg_alpha': 61, 'reg_lambda': 0.364330131933732}\n",
      "[08:02:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.212, val loss per horse: 0.217 Train Accuracy: 26.7%, Val Accuracy: 21.3%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)       \n",
      "Mean Predicted probas of actual race result: 11.938% (Random: 9.881%, Odds: 20.823%)  \n",
      "On 2196 races with 10 horses,R² of winning model: 0.05, R² of odds: 0.19, [R² of random model: -0.15 (should be closed to 0)]\n",
      "R²: 0.054, {'colsample_bytree': 0.26137878810460063, 'gamma': 1.057461802813398, 'max_depth': 4, 'min_child_weight': 9, 'n_estimators': 38, 'reg_alpha': 77, 'reg_lambda': 0.555211195359059}\n",
      "[08:03:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.215, val loss per horse: 0.217 Train Accuracy: 24.5%, Val Accuracy: 21.3%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)       \n",
      "Mean Predicted probas of actual race result: 11.906% (Random: 10.153%, Odds: 20.823%) \n",
      "On 2196 races with 10 horses,R² of winning model: 0.05, R² of odds: 0.19, [R² of random model: -0.12 (should be closed to 0)]\n",
      "R²: 0.053, {'colsample_bytree': 0.31818498036946563, 'gamma': 5.609625922964084, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 125, 'reg_alpha': 41, 'reg_lambda': 0.5002901503335936}\n",
      "[08:05:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.227, val loss per horse: 0.227 Train Accuracy: 19.1%, Val Accuracy: 17.6%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)       \n",
      "Mean Predicted probas of actual race result: 10.316% (Random: 9.935%, Odds: 20.823%)  \n",
      "On 2196 races with 10 horses,R² of winning model: 0.01, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.011, {'colsample_bytree': 0.31583059192749313, 'gamma': 5.3816379980450595, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 32, 'reg_alpha': 123, 'reg_lambda': 0.6214850011300186}\n",
      "[08:06:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.228, val loss per horse: 0.228 Train Accuracy: 18.4%, Val Accuracy: 17.1%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)       \n",
      "Mean Predicted probas of actual race result: 10.213% (Random: 9.810%, Odds: 20.823%)  \n",
      "On 2196 races with 10 horses,R² of winning model: 0.01, R² of odds: 0.19, [R² of random model: -0.14 (should be closed to 0)]\n",
      "R²: 0.0075, {'colsample_bytree': 0.5302233434741355, 'gamma': 4.7615653168295164, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 107, 'reg_alpha': 136, 'reg_lambda': 0.7859944265819634}\n",
      "[08:08:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.229, val loss per horse: 0.229 Train Accuracy: 17.2%, Val Accuracy: 15.8%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)       \n",
      "Mean Predicted probas of actual race result: 10.123% (Random: 10.215%, Odds: 20.823%) \n",
      "On 2196 races with 10 horses,R² of winning model: 0.00, R² of odds: 0.19, [R² of random model: -0.12 (should be closed to 0)]\n",
      "R²: 0.004, {'colsample_bytree': 0.5417476233138829, 'gamma': 6.250129628597695, 'max_depth': 9, 'min_child_weight': 7, 'n_estimators': 60, 'reg_alpha': 137, 'reg_lambda': 0.38282852625031305}\n",
      "[08:11:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.228, val loss per horse: 0.228 Train Accuracy: 19.2%, Val Accuracy: 16.3%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)       \n",
      "Mean Predicted probas of actual race result: 10.229% (Random: 10.070%, Odds: 20.823%) \n",
      "On 2196 races with 10 horses,R² of winning model: 0.01, R² of odds: 0.19, [R² of random model: -0.12 (should be closed to 0)]\n",
      "R²: 0.0081, {'colsample_bytree': 0.7508093621357907, 'gamma': 2.8769512686632046, 'max_depth': 7, 'min_child_weight': 9, 'n_estimators': 72, 'reg_alpha': 150, 'reg_lambda': 0.908716793205029}\n",
      "[08:14:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.225, val loss per horse: 0.225 Train Accuracy: 20.1%, Val Accuracy: 18.6%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 10.549% (Random: 9.921%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.02, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.019, {'colsample_bytree': 0.3612684892337149, 'gamma': 6.388031931584048, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 141, 'reg_alpha': 102, 'reg_lambda': 0.12326672982105258}\n",
      "[08:16:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.228, val loss per horse: 0.228 Train Accuracy: 18.4%, Val Accuracy: 17.3%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 10.224% (Random: 10.016%, Odds: 20.823%)  \n",
      "On 2196 races with 10 horses,R² of winning model: 0.01, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.0079, {'colsample_bytree': 0.8937567119814034, 'gamma': 8.39633368752731, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 62, 'reg_alpha': 117, 'reg_lambda': 0.9600937929681767}\n",
      "[08:18:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.220, val loss per horse: 0.221 Train Accuracy: 22.5%, Val Accuracy: 19.9%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 11.174% (Random: 9.825%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.04, R² of odds: 0.19, [R² of random model: -0.14 (should be closed to 0)]\n",
      "R²: 0.038, {'colsample_bytree': 0.5158043201081668, 'gamma': 6.296320975779742, 'max_depth': 8, 'min_child_weight': 8, 'n_estimators': 155, 'reg_alpha': 68, 'reg_lambda': 0.22844060500504082}\n",
      "[08:22:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.225, val loss per horse: 0.225 Train Accuracy: 20.1%, Val Accuracy: 18.5%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 10.549% (Random: 9.994%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.02, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.019, {'colsample_bytree': 0.7167643038992311, 'gamma': 5.725569715703959, 'max_depth': 7, 'min_child_weight': 0, 'n_estimators': 164, 'reg_alpha': 104, 'reg_lambda': 0.49231540323088674}\n",
      "[08:28:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.215, val loss per horse: 0.220 Train Accuracy: 27.9%, Val Accuracy: 20.1%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 11.343% (Random: 10.230%, Odds: 20.823%)  \n",
      "On 2196 races with 10 horses,R² of winning model: 0.04, R² of odds: 0.19, [R² of random model: -0.12 (should be closed to 0)]\n",
      "R²: 0.04, {'colsample_bytree': 0.7569898325742117, 'gamma': 1.5106819354395666, 'max_depth': 9, 'min_child_weight': 7, 'n_estimators': 103, 'reg_alpha': 103, 'reg_lambda': 0.17836846158526343}\n",
      "[08:32:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.228, val loss per horse: 0.228 Train Accuracy: 18.5%, Val Accuracy: 17.4%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 10.249% (Random: 9.948%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.01, R² of odds: 0.19, [R² of random model: -0.12 (should be closed to 0)]\n",
      "R²: 0.0088, {'colsample_bytree': 0.9133095415002648, 'gamma': 8.283513867867965, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 143, 'reg_alpha': 114, 'reg_lambda': 0.3532272463205678}\n",
      "[08:35:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.225, val loss per horse: 0.226 Train Accuracy: 20.6%, Val Accuracy: 18.8%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 10.469% (Random: 9.890%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.02, R² of odds: 0.19, [R² of random model: -0.14 (should be closed to 0)]\n",
      "R²: 0.017, {'colsample_bytree': 0.9282232246877906, 'gamma': 3.2102921237043356, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 97, 'reg_alpha': 127, 'reg_lambda': 0.5985023792773377}\n",
      "[08:39:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.229, val loss per horse: 0.229 Train Accuracy: 16.4%, Val Accuracy: 15.3%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 10.097% (Random: 9.784%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.00, R² of odds: 0.19, [R² of random model: -0.14 (should be closed to 0)]\n",
      "R²: 0.003, {'colsample_bytree': 0.7586751964309264, 'gamma': 5.183053951046905, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 71, 'reg_alpha': 146, 'reg_lambda': 0.2971084437816387}\n",
      "[08:42:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.223, val loss per horse: 0.223 Train Accuracy: 21.2%, Val Accuracy: 18.9%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 10.827% (Random: 9.980%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.03, R² of odds: 0.19, [R² of random model: -0.12 (should be closed to 0)]\n",
      "R²: 0.028, {'colsample_bytree': 0.9647094933926168, 'gamma': 8.288858781983087, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 28, 'reg_alpha': 76, 'reg_lambda': 0.7494186208272151}\n",
      "[08:43:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.230, val loss per horse: 0.230 Train Accuracy: 9.9%, Val Accuracy: 10.4%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 9.988% (Random: 9.890%, Odds: 20.823%)    \n",
      "On 2196 races with 10 horses,R² of winning model: -0.00, R² of odds: 0.19, [R² of random model: -0.14 (should be closed to 0)]\n",
      "R²: -0.0014, {'colsample_bytree': 0.3858630900519838, 'gamma': 8.901486758276523, 'max_depth': 5, 'min_child_weight': 10, 'n_estimators': 146, 'reg_alpha': 170, 'reg_lambda': 0.7441672579029956}\n",
      "[08:46:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.216, val loss per horse: 0.218 Train Accuracy: 23.7%, Val Accuracy: 20.7%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 11.744% (Random: 9.927%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.05, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.051, {'colsample_bytree': 0.22252554887933187, 'gamma': 1.080056125366963, 'max_depth': 2, 'min_child_weight': 4, 'n_estimators': 176, 'reg_alpha': 86, 'reg_lambda': 0.01944612011268826}\n",
      "[08:47:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.213, val loss per horse: 0.216 Train Accuracy: 24.7%, Val Accuracy: 21.9%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 12.316% (Random: 9.983%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.06, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.061, {'colsample_bytree': 0.43448881078626334, 'gamma': 1.8810808748424388, 'max_depth': 2, 'min_child_weight': 9, 'n_estimators': 47, 'reg_alpha': 51, 'reg_lambda': 0.038996044159775534}\n",
      "[08:48:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.213, val loss per horse: 0.216 Train Accuracy: 24.5%, Val Accuracy: 22.0%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)        \n",
      "Mean Predicted probas of actual race result: 12.199% (Random: 9.867%, Odds: 20.823%)   \n",
      "On 2196 races with 10 horses,R² of winning model: 0.06, R² of odds: 0.19, [R² of random model: -0.14 (should be closed to 0)]\n",
      "R²: 0.059, {'colsample_bytree': 0.4399991818346859, 'gamma': 2.1785902282536402, 'max_depth': 2, 'min_child_weight': 10, 'n_estimators': 90, 'reg_alpha': 52, 'reg_lambda': 0.01765421495247255}\n",
      "[08:49:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.216, val loss per horse: 0.219 Train Accuracy: 24.4%, Val Accuracy: 21.0%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)       \n",
      "Mean Predicted probas of actual race result: 11.624% (Random: 9.948%, Odds: 20.823%)  \n",
      "On 2196 races with 10 horses,R² of winning model: 0.05, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.048, {'colsample_bytree': 0.6511910274681174, 'gamma': 3.906288707024739, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 47, 'reg_alpha': 59, 'reg_lambda': 0.11634340907986569}\n",
      "[08:50:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.193, val loss per horse: 0.214 Train Accuracy: 37.5%, Val Accuracy: 21.7%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)         \n",
      "Mean Predicted probas of actual race result: 12.934% (Random: 9.947%, Odds: 20.823%)    \n",
      "On 2196 races with 10 horses,R² of winning model: 0.06, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.065, {'colsample_bytree': 0.48257614378658514, 'gamma': 2.040071467745888, 'max_depth': 10, 'min_child_weight': 0, 'n_estimators': 83, 'reg_alpha': 41, 'reg_lambda': 0.0038565202931657647}\n",
      "[08:53:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.210, val loss per horse: 0.217 Train Accuracy: 27.9%, Val Accuracy: 21.4%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)          \n",
      "Mean Predicted probas of actual race result: 12.142% (Random: 10.054%, Odds: 20.823%)    \n",
      "On 2196 races with 10 horses,R² of winning model: 0.06, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.057, {'colsample_bytree': 0.6248784857362564, 'gamma': 4.108314834404679, 'max_depth': 10, 'min_child_weight': 0, 'n_estimators': 83, 'reg_alpha': 41, 'reg_lambda': 0.24083416194236437}\n",
      "[08:57:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training for 10 horses (13625 races): loss per horse: 0.218, val loss per horse: 0.220 Train Accuracy: 24.6%, Val Accuracy: 20.0%\n",
      "\n",
      "Comparing on same races w/ 10 horses with odds 2196 races (3182 races in total)          \n",
      "Mean Predicted probas of actual race result: 11.296% (Random: 9.941%, Odds: 20.823%)     \n",
      "On 2196 races with 10 horses,R² of winning model: 0.04, R² of odds: 0.19, [R² of random model: -0.13 (should be closed to 0)]\n",
      "R²: 0.04, {'colsample_bytree': 0.46748034774953245, 'gamma': 2.55091440153943, 'max_depth': 6, 'min_child_weight': 0, 'n_estimators': 125, 'reg_alpha': 90, 'reg_lambda': 0.1023662216532309}\n",
      "[08:59:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 27%|██▋       | 27/100 [1:09:17<3:01:48, 149.44s/trial, best loss: -0.06495834261153755]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture --no-stdout\n",
    "trials = Trials()\n",
    "best = fmin(fn=hyperparameter_tuning,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials,\n",
    "           verbose=True)\n",
    "\n",
    "print (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
